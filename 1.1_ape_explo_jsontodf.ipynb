{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to explore a way to analyse site text to identify activities of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  base_scheme                     base_url                               dom  \\\n",
      "3      [http]          [www.calorbois.com]  [R08-poêle, R08-Entretien poêle]   \n",
      "4      [http]  [www.samson-couverture.com]                    [R08-cheminée]   \n",
      "\n",
      "                                          image_urls  \\\n",
      "3  [http://www.calorbois.com/img/logo/logo.jpg, h...   \n",
      "4  [http://www.samson-couverture.com/images/logoc...   \n",
      "\n",
      "                                              images             lib_arty  \\\n",
      "3  [{'url': 'http://www.calorbois.com/img/logo/lo...                  NaN   \n",
      "4  [{'url': 'http://www.samson-couverture.com/ima...  [SAMSON COUVERTURE]   \n",
      "\n",
      "                                siret  \\\n",
      "3                                 NaN   \n",
      "4  [http://www.samson-couverture.com]   \n",
      "\n",
      "                                                 url  \\\n",
      "3  [http://www.calorbois.com/mentions%20legales.h...   \n",
      "4                 [http://www.samson-couverture.com]   \n",
      "\n",
      "                                 website_description           website_email  \\\n",
      "3  [mentions légales sites www.calorbois.com et w...  [calor.bois@orange.fr]   \n",
      "4  [nombreux services pour tout ce qui concerne l...                      []   \n",
      "\n",
      "                                    website_keywords  \\\n",
      "3                                                 []   \n",
      "4  [couverture, zinguerie,ardoises,cheminées,réfe...   \n",
      "\n",
      "                        website_phone     website_siren website_siret  \\\n",
      "3  [ 53180703000014,  07.70.69.85.00]  [53180703000014]   [531807030]   \n",
      "4  [ 02 41 34 79 44,  02 41 60 28 21]                []            []   \n",
      "\n",
      "                                     website_tag_div  \\\n",
      "3  [                                             ...   \n",
      "4  [           les fougères rd 94 route de brioll...   \n",
      "\n",
      "                                       website_tag_p  \\\n",
      "3  [              philippe loquin               5...   \n",
      "4                                                 []   \n",
      "\n",
      "                                    website_tag_span  \\\n",
      "3                               [© calor bois 2014 ]   \n",
      "4  [rien     entreprise de couverture à angers, m...   \n",
      "\n",
      "                                      website_titles  \n",
      "3     [<title>calor bois - mentions légales</title>]  \n",
      "4  [<title>samson couverture zinguerie à angers, ...  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "\n",
    "# function to convert json file to dataframe\n",
    "def json2df(file):\n",
    "    tweets = []\n",
    "    for line in open(file, encoding=\"utf8\"):\n",
    "        tweets.append(json.loads(line))\n",
    "\n",
    "    df = pd.DataFrame(tweets)\n",
    "    return df\n",
    "\n",
    "alljsonsites_filename = 'scraping_websites_parse_workers_webites_results.jl'\n",
    "dirpath = os.getcwd()\n",
    "data_foldername = dirpath+ \"\\data\"\n",
    "alljsonsites_filename_path = data_foldername+\"\\\\\"+alljsonsites_filename \n",
    "\n",
    "df = json2df(alljsonsites_filename_path)\n",
    "\n",
    "print(df.tail(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " html &zzz;   é tags &zzz &zz: will be&replaced with space. NOT this &abc\n"
     ]
    }
   ],
   "source": [
    "#On ajoute une colonne avec les colonnes de texte qui nous intéresse\n",
    "import re\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "        self.convert_charrefs = True\n",
    "        self.entityref = re.compile('&[a-zA-Z][-.a-zA-Z0-9]*[^a-zA-Z0-9]')\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        #remplace les tags de début par un espace\n",
    "        self.fed.append(' ')\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        #remplace les tags de fin par un espace\n",
    "        self.fed.append(' ')\n",
    "\n",
    "    def handle_entityref(self, name):\n",
    "        if html.parser.get(name) is None:\n",
    "            m = self.html.parser.match(self.rawdata.splitlines()[self.lineno-1][self.offset:])\n",
    "            entity = m.group()\n",
    "            # semicolon is consumed, other chars are not.\n",
    "            if entity[-1] != ';':\n",
    "                entity = entity[:-1]\n",
    "            self.fed.append(entity)\n",
    "        else:\n",
    "            self.fed.append(' ')\n",
    "\n",
    "    def get_data(self):\n",
    "        self.close()    # N.B. ensure all buffered data has been processed\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MyHTMLParser()\n",
    "    s.feed(html)\n",
    "    data = s.get_data()\n",
    "    s.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "print(strip_tags('<title>html &zzz; <div> &eacute; tags<p>&zzz &zz: will be&amp;replaced</p>with space. NOT this &abc'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [' calor bois - philippe loquin - accueil ']\n",
      "1                            [' froid systemservice ']\n",
      "2    [' calor bois - philippe loquin - contact et c...\n",
      "3                  [' calor bois - mentions légales ']\n",
      "4    [' samson couverture zinguerie à angers, maine...\n",
      "Name: website_titles, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# convertit dans le dataframe \n",
    "df['website_titles']=df['website_titles'].apply(lambda x: strip_tags(str(x)))\n",
    "\n",
    "print(df['website_titles'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description :  ['nombreux services pour tout ce qui concerne la toiture, en création, rénovation et entretien']\n",
      "token de cette description :  ['service', 'concerner', 'toiture', 'création', 'rénovation', 'entretien']\n"
     ]
    }
   ],
   "source": [
    "# Start text classification\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import string\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = French()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "unephrase=str(df['website_description'].iloc[4])\n",
    "print('description : ',unephrase)\n",
    "print('token de cette description : ', spacy_tokenizer(unephrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    text = ''.join(text)\n",
    "\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "# Vectorization Feature Engineering (TF-IDF)\n",
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 0 1]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#Splitting The Data into Training and Test Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "X = df['website_description']\n",
    "\n",
    "ylabels = df['dom']\n",
    "ylabels = MultiLabelBinarizer().fit_transform(ylabels)\n",
    "print(ylabels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x00000000186EB588>),\n",
       "                ('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=No...\n",
       "                                 vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200,\n",
       "                                                   class_weight=None, coef0=0.0,\n",
       "                                                   decision_function_shape='ovr',\n",
       "                                                   degree=3,\n",
       "                                                   gamma='auto_deprecated',\n",
       "                                                   kernel='linear', max_iter=-1,\n",
       "                                                   probability=True,\n",
       "                                                   random_state=<mtrand.RandomState object at 0x000000001A5D4798>,\n",
       "                                                   shrinking=True, tol=0.001,\n",
       "                                                   verbose=False),\n",
       "                                     n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a Pipeline and Generating the Model\n",
    "# Run classifier\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=random_state))\n",
    " \n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
